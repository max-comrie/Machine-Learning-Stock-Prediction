# -*- coding: utf-8 -*-
"""Stock Predictor LTSM Final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bVfgkoUr2DLoEUtFhKTfIyPiBN-qIcLu

# Importing Modules and Data Preprocessing
"""

# Commented out IPython magic to ensure Python compatibility.

import os
import tqdm
from datetime import datetime
import requests

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.models import Sequential, Model # type: ignore
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input # type: ignore
from sklearn.model_selection import TimeSeriesSplit
from scikeras.wrappers import KerasClassifier # type: ignore
from tensorflow.keras.callbacks import EarlyStopping #type: ignore
import pandas_ta as ta
import joblib

'''
def fetch_stock_data(ticker):
    API_KEY = "d0sru6pr01qid5qb1km0d0sru6pr01qid5qb1kmg"  # â† Replace this with your real key
    url = f"https://finnhub.io/api/v1/stock/candle?symbol={ticker}&resolution=D&from=946684800&to={int(datetime.now().timestamp())}&token={API_KEY}"
    
    res = requests.get(url)
    data = res.json()
    
    if data.get("s") != "ok":
        raise ValueError("API error or rate limit")

    df = pd.DataFrame({
        "Date": pd.to_datetime(data["t"], unit="s"),
        "Open": data["o"],
        "High": data["h"],
        "Low": data["l"],
        "Close": data["c"],
        "Volume": data["v"],
    })

    df.set_index("Date", inplace=True)
    return df
'''
def fetch_stock_data_polygon(ticker):
    API_KEY = "vichhOoRa9YbkUt8YsNYKh4mnEQhVGXu"
    start_date = "2020-01-01"
    end_date = datetime.now().strftime("%Y-%m-%d")

    url = (
        f"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/"
        f"{start_date}/{end_date}"
        f"?adjusted=true&sort=asc&limit=50000&apiKey={API_KEY}"
    )

    res = requests.get(url)
    data = res.json()

    if res.status_code != 200 or "results" not in data:
        raise Exception(f"Polygon API error: {data}")

    df = pd.DataFrame(data["results"])
    df["timestamp"] = pd.to_datetime(df["t"], unit="ms")
    df.set_index("timestamp", inplace=True)

    # Rename columns to standard ones
    df.rename(columns={
        "o": "Open",
        "h": "High",
        "l": "Low",
        "c": "Close",
        "v": "Volume"
    }, inplace=True)

    return df[["Open", "High", "Low", "Close", "Volume"]]

def fetch_company_metrics(ticker):
    API_KEY = "d0tibe1r01qlvahcasq0d0tibe1r01qlvahcasqg"
    url = f"https://finnhub.io/api/v1/stock/candle?symbol={ticker}&metric=all&token={API_KEY}"
    res = requests.get(url)
    data = res.json()

    metrics = data.get("metric", {})
    return {
        "EPS": metrics.get("epsTrailingTwelveMonths"),
        "PE_Ratio": metrics.get("peBasicExclExtraTTM"),
        "Dividend_Yield": metrics.get("dividendYieldIndicatedAnnual"),
        "Market_Cap": metrics.get("marketCapitalization"),
        "52_Week_High": metrics.get("52WeekHigh"),
        "52_Week_Low": metrics.get("52WeekLow"),
    }



def train_model(ticker):
    stock = yf.Ticker(ticker)
    
    # 1ï¸âƒ£ Download historical stock data with actions (dividends & splits)
    aapl_df = fetch_stock_data_polygon(ticker)
    #aapl_df = yf.download(ticker, start="2000-01-01", end=None, actions=True)
    #aapl_df = aapl_df.ta.ticker(ticker)


    

    metrics = fetch_company_metrics(ticker)


    '''
    # 3ï¸âƒ£ Get company info
    info = stock.info
    '''
    company_info = {
    "EPS": metrics["EPS"],
    "PE Ratio": metrics["PE_Ratio"],
    "Dividend Yield": metrics["Dividend_Yield"],
    "Market Cap": metrics["Market_Cap"],
    "52-Week High": metrics["52_Week_High"],
    "52-Week Low": metrics["52_Week_Low"]
    }

    


    '''
    # 4ï¸âƒ£ Get financial statements (balance sheet, income statement, cash flow)
    income_statement = pd.read_csv(f"/content/Stock_Predictor_Project_Files/{ticker.lower()}_income_statement.csv")
    balance_sheet_assets = pd.read_csv(f"/content/Stock_Predictor_Project_Files/{ticker.lower()}_balance_sheet_assets.csv")
    balance_sheet_equity = pd.read_csv(f"/content/Stock_Predictor_Project_Files/{ticker.lower()}_balance_sheet_equity.csv")
    balance_sheet_liabilities = pd.read_csv(f"/content/Stock_Predictor_Project_Files/{ticker.lower()}_balance_sheet_liabilities.csv")
    cash_flow_statement = pd.read_csv(f"/content/Stock_Predictor_Project_Files/{ticker.lower()}_cash_flow_statement.csv")
    



    financials = {
        "Income Statement": income_statement,
        "Balance Sheet Assets": balance_sheet_assets,
        "Balance Sheet Equity": balance_sheet_equity,
        "Balance Sheet Liabilities": balance_sheet_liabilities,
        "Cash Flow": cash_flow_statement
    }
    '''

    
    # 5ï¸âƒ£ Save everything to an Excel file
    #with pd.ExcelWriter(rf"C:\Users\maxco\Documents\Personal Projects\Stock Predictor Project\{ticker}_stock_data.xlsx") as writer:
    #    aapl_df.to_excel(writer, sheet_name="Stock Prices")  # Historical stock data
    #    pd.DataFrame([company_info]).to_excel(writer, sheet_name="Company Info")  # Company details
    #    for key, df in financials.items():
    #        df.to_excel(writer, sheet_name=key)  # Financial statements

    '''
    # Flatten the multiindex columns to single-level column names
    # by joining the levels with an underscore.
    aapl_df.columns = ['_'.join(col) for col in aapl_df.columns]

    # Remove the ticker symbol ('AAPL') from the column names
    # (if you want to keep just the column names).
    aapl_df.columns = [col.replace('_AAPL', '') for col in aapl_df.columns]
    aapl_df = aapl_df.rename(columns={'Date_': 'Date'})
    '''

    
    # Adding Features
    #print(f" AAPL_DF PRINTED! {aapl_df}")
    print(f"METRICS {metrics}")
    aapl_df["10_day_MA"] = aapl_df["Close"].rolling(window=10, min_periods=1).mean()
    aapl_df["5_day_MA"] = aapl_df["Close"].rolling(window=5, min_periods=1).mean()
    aapl_df["3_day_MA"] = aapl_df["Close"].rolling(window=3, min_periods=1).mean()

    '''
    Example with Steps Counter
    ðŸ“Š SMA (Simple Moving Average) - Abrupt Changes

    Day 20: Average is based on steps from days 1-20.
    Day 21: We drop day 1's steps completely and add day 21â€™s.
    If day 1 had really low steps, the new average suddenly jumps up when we drop it.
    ðŸ“ˆ RMA (Rolling Moving Average) - Smoother Change


    Instead of dropping day 1 completely, we gradually reduce its influence while adding more weight to new days.
    The change in average is much smoother day to day.
    '''



    aapl_df["1_day_pct_change"] = aapl_df["Close"].pct_change()*100
    aapl_df["3_day_pct_change"] = aapl_df["Close"].pct_change(periods=3)*100
    aapl_df["5_day_pct_change"] = aapl_df["Close"].pct_change(periods=5)*100
    aapl_df["10_day_pct_change"] = aapl_df["Close"].pct_change(periods=10)*100

    # Market Cap
    #aapl_df["Market Cap"] = aapl_df["Close"] * financials["Balance Sheet Equity"]["shares_outstanding"]
    '''
    # Earnings Per Share (EPS)
    aapl_df['EPS'] = metrics['trailingEps']  # Using trailingEps from stock.info

    # Price-to-Earnings Ratio (P/E)
    aapl_df['PE_Ratio'] = metrics['trailingPE']  # Using trailingPE from stock.info

    # Dividend Yield
    aapl_df['Dividend_Yield'] = metrics['dividendYield'] # Using dividendYield from stock.info
    '''

    # Relative Strength Index
    aapl_df['RSI'] = ta.rsi(close=aapl_df['Close'], window=14)


    # Calculate MACD
    macd_df = ta.macd(close=aapl_df['Close'])

    # Add MACD values to aapl_df
    aapl_df['MACD'] = macd_df['MACD_12_26_9']  # Assuming default parameters
    aapl_df['MACD_Signal'] = macd_df['MACDs_12_26_9']
    aapl_df['MACD_Hist'] = macd_df['MACDh_12_26_9']

    # Bollinger Bands calculation
    bbands_df = ta.bbands(close=aapl_df['Close']) # might be better to use EMA instead of SMA

    # Add Bollinger Band columns to aapl_df
    aapl_df['BBL'] = bbands_df['BBL_5_2.0']  # Lower Band
    aapl_df['BBM'] = bbands_df['BBM_5_2.0']  # Middle Band
    aapl_df['BBU'] = bbands_df['BBU_5_2.0']  # Upper Band

    '''
    ðŸ”¥ How It Affects Bollinger Bands
    EMA-based Bollinger Bands react faster to price changes ðŸ“ˆðŸ“‰
    SMA-based Bollinger Bands are smoother and respond slower
    ðŸ’¡ EMA is better for short-term trends, while SMA is better for long-term analysis
    '''

    # Average True Range
    aapl_df['ATR'] = ta.atr(high=aapl_df['High'], low=aapl_df['Low'], close=aapl_df['Close'])

    #Schochastic Oscillator
    stoch_df = ta.stoch(high=aapl_df['High'], low=aapl_df['Low'], close=aapl_df['Close'])

    # Get the stochastic oscillator values from the DataFrame returned by ta.stoch
    aapl_df['SlowK'] = stoch_df['STOCHk_14_3_3']
    aapl_df['SlowD'] = stoch_df['STOCHd_14_3_3']

    #On-Balance Volume
    aapl_df['OBV'] = ta.obv(close=aapl_df['Close'], volume=aapl_df['Volume'])

    '''
    # Plot closing prices over time
    plt.figure(figsize=(10, 5))
    plt.plot(aapl_df.index, aapl_df["Close"], label="AAPL Closing Price")
    plt.xlabel("Date")
    plt.ylabel("Price (USD)")
    plt.title("Apple Stock Price Over Time")
    plt.legend()
    plt.show()

    # 1 day pct change over time
    plt.figure(figsize=(10, 5))
    plt.plot(aapl_df.index, aapl_df["1_day_pct_change"], label="AAPL 1 Day % Change")
    plt.xlabel("Date")
    plt.ylabel("Price (USD)")
    plt.title("Apple Stock Price Over Time")
    plt.legend()
    plt.show()

    # OBV over time
    plt.figure(figsize=(10, 5))
    plt.plot(aapl_df.index, aapl_df["OBV"], label="On-Balance Volume")
    plt.xlabel("Date")
    plt.ylabel("Price (USD)")
    plt.title("Apple Stock Price Over Time")
    plt.legend()
    plt.show()

    #LSTM: Dynamic model that trains on various time intervals

    """# Grouping/Plotting"""

    aapl_df.dropna(inplace=True)  # Removes rows with NaN values

    sns.relplot(x="Close", y="Volume", kind="scatter", data=aapl_df)

    sns.relplot(x="ATR", y="Close", kind="scatter", data=aapl_df)

    #sns.relplot(x="Stock Splits", y="1_day_pct_change", kind="scatter", data=aapl_df)

    plt.figure(figsize=(24, 12))  # Adjust figure size as needed
    plt.plot(aapl_df.index, aapl_df["Close"], label="Close")
    plt.plot(aapl_df.index, aapl_df["10_day_MA"], label="10-day MA")
    plt.plot(aapl_df.index, aapl_df["5_day_MA"], label="5-day MA")
    plt.plot(aapl_df.index, aapl_df["3_day_MA"], label="3-day MA")
    plt.xlabel("Date")
    plt.xlim(datetime(2024,1,1),datetime.now())
    plt.ylabel("Price")
    plt.title("Apple Stock Price with Moving Averages")
    plt.legend()
    plt.grid(True)
    plt.show()

    correlation_matrix = aapl_df[['Close', 'Volume', 'RSI', 'MACD', 'ATR']].corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
    plt.show()
    #NOTES:
    # High correlation between Close and ATR
    # Moderate negative correlation between close and volume
    # Moderate correlation between MACD and RSI
    # Moderate negative correlation between volume and ATR

    sns.pairplot(aapl_df[['Close', 'Volume', 'RSI', 'MACD', 'ATR']])
    plt.show()

    plt.figure(figsize=(12, 6))  # Adjust figure size as needed
    plt.plot(aapl_df.index, aapl_df["ATR"], label="ATR")
    plt.plot(aapl_df.index, aapl_df["BBL"], label="Lower Bollinger Band")
    plt.plot(aapl_df.index, aapl_df["BBM"], label="Middle Bollinger Band")
    plt.plot(aapl_df.index, aapl_df["BBU"], label="Upper Bollinger Band")
    plt.xlabel("Date")
    plt.ylabel("Value")
    plt.xlim(datetime(2024,1,1),datetime.now())
    plt.title("Volatility Indicators: ATR and Bollinger Bands")
    plt.legend()
    plt.grid(True)
    plt.show()

    plt.figure(figsize=(12, 6))  # Adjust figure size as needed
    plt.plot(aapl_df.index, aapl_df["SlowK"], label="%K line")
    plt.plot(aapl_df.index, aapl_df["SlowD"], label="%D line")
    plt.xlabel("Date")
    plt.ylabel("Value")
    plt.title("Stochastic Oscillators")
    plt.legend()
    plt.grid(True)
    plt.show()

    fig, plts = plt.subplots(nrows=1, ncols=2, figsize=(10, 6))  # Adjust figure size as needed
    plts[0].hist(aapl_df["1_day_pct_change"].loc[datetime(2024,1,1):datetime.now()], bins=30, alpha=0.6, label="1-day")
    plts[0].hist(aapl_df["3_day_pct_change"].loc[datetime(2024,1,1):datetime.now()], bins=30, alpha=0.6, label="3-day")
    plts[0].hist(aapl_df["5_day_pct_change"].loc[datetime(2024,1,1):datetime.now()], bins=30, alpha=0.6, label="5-day")
    plts[0].hist(aapl_df["10_day_pct_change"].loc[datetime(2024,1,1):datetime.now()], bins=30, alpha=0.6, label="10-day")
    plts[0].set_xlabel("Percentage Change")
    plts[0].set_ylabel("Frequency")
    plts[0].set_title("Distribution of Percentage Changes")
    plts[0].legend()
    plts[0].grid(True)


    plts[1].hist(aapl_df["1_day_pct_change"], bins=30, alpha=0.6, label="1-day")
    plts[1].hist(aapl_df["3_day_pct_change"], bins=30, alpha=0.6, label="3-day")
    plts[1].hist(aapl_df["5_day_pct_change"], bins=30, alpha=0.6, label="5-day")
    plts[1].hist(aapl_df["10_day_pct_change"], bins=30, alpha=0.6, label="10-day")
    plts[1].set_xlabel("Percentage Change")
    plts[1].set_ylabel("Frequency")
    plts[1].set_title("Distribution of Percentage Changes")
    plts[1].legend()
    plts[1].grid(True)

    aapl_df['10_day_pct_change'].loc[abs(aapl_df['10_day_pct_change']) >= 30]
    #Most volatile during lower prices and 2008 recession

    aapl_df['ATR'].loc[((aapl_df['ATR'] >= (aapl_df['ATR'].mean())+(3*(aapl_df['ATR'].std()))) | (aapl_df['ATR'] <= aapl_df['ATR'].mean())-(3*(aapl_df['ATR'].std())))]

    data = [aapl_df["1_day_pct_change"],
            aapl_df["3_day_pct_change"],
            aapl_df["5_day_pct_change"],
            aapl_df["10_day_pct_change"]]

    plt.figure(figsize=(10, 6))  # Adjust figure size as needed
    plt.boxplot(data, labels=["1-day", "3-day", "5-day", "10-day"])
    plt.xlabel("Timeframe")
    plt.ylabel("Percentage Change")
    plt.title("Distribution of Percentage Changes")
    plt.grid(True)
    plt.show()

    sns.kdeplot(aapl_df["1_day_pct_change"], label="1-day", fill=True)
    #sns.kdeplot(aapl_df["3_day_pct_change"], label="3-day")
    #sns.kdeplot(aapl_df["5_day_pct_change"], label="5-day")

    aapl_df["1_day_pct_change"].hist(bins=30, alpha=0.5, label="1-day")

    # Create scatter plots for each fundamental indicator vs. Close
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)  # 1 row, 3 columns, first subplot
    plt.scatter(aapl_df['EPS'], aapl_df['Close'])
    plt.xlabel('EPS')
    plt.ylabel('Close Price')
    plt.title('EPS vs. Close Price')

    plt.subplot(1, 3, 2)  # 1 row, 3 columns, second subplot
    plt.scatter(aapl_df['PE_Ratio'], aapl_df['Close'])
    plt.xlabel('PE Ratio')
    plt.ylabel('Close Price')
    plt.title('PE Ratio vs. Close Price')

    plt.subplot(1, 3, 3)  # 1 row, 3 columns, third subplot
    plt.scatter(aapl_df['Dividend_Yield'], aapl_df['Close'])
    plt.xlabel('Dividend Yield')
    plt.ylabel('Close Price')
    plt.title('Dividend Yield vs. Close Price')

    plt.tight_layout()  # Adjust spacing between subplots
    plt.show()
    '''

    """# Target Feature Analysis"""

    aapl_df["Price Movement"] = np.where(aapl_df["1_day_pct_change"] > 0, 1, 0) # TARGET FEATURE

    #aapl_df.groupby(['Stock Splits'], as_index=False)['1_day_pct_change'].mean()

    technical_features = [
        '10_day_MA', '5_day_MA', '3_day_MA',
        '1_day_pct_change', '3_day_pct_change', '5_day_pct_change', '10_day_pct_change',
        'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist',
        'BBL', 'BBM', 'BBU',
        'ATR', 'SlowK', 'SlowD',
        'OBV'
    ]


    '''
    correlation_matrix = aapl_df[technical_features].iloc[:250].corr()


    plt.figure(figsize=(16, 12))  # Adjust figure size as needed
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('Correlation Matrix of Technical Analysis Features')
    plt.show()
    '''

    """# Training"""

    # ---- 1. Data Preprocessing ----
    aapl_df.dropna(inplace=True)
    scaler = MinMaxScaler()
    aapl_df_scaled = aapl_df.copy()  # Avoid modifying the original DataFrame
    aapl_df_scaled[technical_features] = scaler.fit_transform(aapl_df_scaled[technical_features])
    #aapl_df_scaled_s = aapl_df_scaled.sample(frac=0.2,random_state=42)


    # ---- 2. Sequence Creation ----
    def create_sequences(df, seq_length=40):
        X, y = [], []
        for i in range(len(df) - seq_length):
            X.append(df.iloc[i:i+seq_length][technical_features].to_numpy())  # Convert explicitly
            y.append(df.iloc[i+seq_length]['Price Movement'])  # Ensure this is 1D
        return np.array(X), np.array(y)

    seq_length = 40
    X,y = create_sequences(aapl_df_scaled)
    #X =  X_null[~np.isnan(X_null)]
    #y = y_null[~np.isnan(y_null)] 


    # 3. Train-Test Split

    print(f"AAPL_DF SUMMARY:\n {aapl_df.info}")
    print(f"HERE IS X:\n {X}")
    print(f"HERE IS y:\n {y}")

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)


    #  4. Build LSTM Model
    lstm_model = Sequential([
        LSTM(64, input_shape=(seq_length, len(technical_features)),
            return_sequences=True, dropout=0.2, recurrent_dropout=0.2),
        LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')  # Binary classification
    ])

    #  5. Feature Extraction from LSTM

    # Access the input layer directly:
    input_layer = lstm_model.layers[0].input # Get the input layer
    output_layer = lstm_model.layers[-2].output # Get the output layer


    # Define the feature extractor model:
    feature_extractor = Model(inputs=input_layer, outputs=output_layer)
    print(f"feature_extractor {feature_extractor.summary()}")

    # Build and compile the model:
    lstm_model.compile(optimizer='adam', loss='binary_crossentropy') # Build implicitly

    early_stop = EarlyStopping(
    monitor='loss',       # What to monitor: usually 'val_loss' or 'val_accuracy'
    patience=50,         # Stop if no improvement after x epochs
    restore_best_weights=True,  # Revert to best model weights (not final epoch)
    verbose=1
)

    lstm_model.fit(X_train, y_train, epochs=1000, batch_size=32,callbacks = [early_stop])


    # Predictions
    X_train_features = feature_extractor.predict(X_train)
    X_test_features = feature_extractor.predict(X_test)

    # Initialize the Random Forest classifier
    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

    # Train the classifier on the extracted LSTM features
    #rf_classifier.fit(X_train_features, y_train)

    #clf = KerasClassifier(model=rf_classifier, epochs=10, batch_size=32)

    param_grid = {
        "n_estimators": [50, 100, 200],
        "max_depth": [5, 10, 20, None],
        "min_samples_split": [2, 5, 10],
        "min_samples_leaf": [1, 2, 4],
        "bootstrap": [True, False],
    }

    grid_search = GridSearchCV(
        estimator=rf_classifier,
        param_grid=param_grid,
        cv=TimeSeriesSplit(n_splits=3),                 # <-- pass the object here
        scoring="accuracy"
    )

    
    grid_search.fit(X_train_features, y_train)

    final_model = grid_search.best_estimator_

    joblib.dump(final_model, 'stock_predictor_final_model.pkl')
    joblib.dump(scaler, "scaler.pkl")
    feature_extractor.save("lstm_feature_extractor.keras")

    # Make predictions on the testing data
    y_probs = final_model.predict_proba(X_test_features)[:,1]
    y_pred = (y_probs > 0.45).astype(int)
    print(f"y_probs={y_probs}")
    print(f"y_pred={y_pred}")
    # Evaluate the model's performance
    confidence = final_model.predict_proba(X_test_features)[0][1]
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy}")

    # Get a more detailed classification report
    print(classification_report(y_test, y_pred))


    # Summarizing results
    results = {
        "ticker":ticker,
        "prediction": "prediction placeholder",
        "confidence":confidence,
        "timestamp":str(datetime.now())

              
              }
    
    return results

    # how to group features together into a linear vector space / feature embedding

